{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sakshamgandhi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sakshamgandhi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import trained word2vec model into gensim, to later import into pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 92.15175294876099 seconds to load word2vec model\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Took {} seconds to load word2vec model\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read word2vec vectors into embedding for pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(w2v_model.vectors)\n",
    "w2v_embedding = nn.Embedding.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV file and print for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/nlp-getting-started/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence_helper(sentence, lemmatizer, regex_filter, stop_words):\n",
    "    ''' \n",
    "        Helper function for preprocessing, cleans each sentence.  \n",
    "        \n",
    "        Input: \n",
    "            - sentence (string)\n",
    "            - lemmatizer (object)\n",
    "            - regex_filter (object)\n",
    "            - stop_words (set)\n",
    "            \n",
    "        Returns: \n",
    "            - sentence_info_dict (dict)\n",
    "                Keys: \n",
    "                    - clean_word_list (doesn't contain hashtags, hyperlinks, stopwords. Everything lemmatized)\n",
    "                    - raw_word_list\n",
    "                    - hashtag_count (for this sentence)\n",
    "    '''\n",
    "    sentence_info_dict = {}\n",
    "    clean_word_list = []\n",
    "    hashtag_count = 0\n",
    "    \n",
    "    sentence_info_dict['raw_word_list'] = sentence.split()\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    word_list = sentence.split()\n",
    "\n",
    "    for word in word_list:\n",
    "        # Remove hashtags\n",
    "        if '#' in word:\n",
    "            hashtag_count +=1\n",
    "            continue     \n",
    "        # Remove hyperlinks\n",
    "        if 'http' in word:\n",
    "            continue\n",
    "        # Remove stop words\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        # Remove non-characters from words\n",
    "        word = regex_filter.sub('', word)\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        # Lemmatize words\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        clean_word_list.append(word)\n",
    "    \n",
    "    sentence_info_dict['clean_word_list'] = clean_word_list\n",
    "    sentence_info_dict['hashtag_count'] = hashtag_count\n",
    "    \n",
    "    return sentence_info_dict\n",
    "\n",
    "\n",
    "def generate_sentence_vector(sentence, w2v_model):\n",
    "    ''' \n",
    "        Given a sentence, return a vector, by averaging word2vec vector for each word\n",
    "        \n",
    "        Input: \n",
    "            - sentence (string)\n",
    "            - w2v_model (word2vec model, gensim)\n",
    "        Return: \n",
    "            - sentence_vector\n",
    "    '''\n",
    "    sentence_vector = np.zeros((300,), dtype='float32')\n",
    "    vector_count = 0 # number of vectors summed up \n",
    "    sentence_list = sentence.split()\n",
    "\n",
    "    for word in sentence_list:\n",
    "        if word in w2v_model.vocab:\n",
    "            sentence_vector = sentence_vector + w2v_model[word]\n",
    "            vector_count += 1\n",
    "    \n",
    "    # elementwise division of sentence vector (to turn it into an average)\n",
    "    sentence_vector = sentence_vector / vector_count\n",
    "    \n",
    "    return sentence_vector\n",
    "\n",
    "\n",
    "def prepare_sequence(sentence, word_vec_model):\n",
    "    \"\"\" \n",
    "        Prep input for LSTM\n",
    "        \n",
    "        Input: \n",
    "            - sentence (string)\n",
    "            - word_vec_model (gensim embedding)\n",
    "        \n",
    "        Returns: \n",
    "            - tensor of word indexes, representing index in word embedding\n",
    "\n",
    "\"\"\"\n",
    "    sentence_list = sentence.split()\n",
    "    idxs = [word_vec_model.vocab[word].index for word in sentence if word in word_vec_model]\n",
    "    if len(idxs) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Helper Functions (this one might not work, but it's not being used so don't worry. Just for reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_list_for_nan(input_list):\n",
    "    '''\n",
    "        Helper function, check if list contains any nan values\n",
    "        Input: \n",
    "            - input_list (list)\n",
    "        Return: \n",
    "            - array_has_nan (boolean)\n",
    "    '''\n",
    "    input_np_array = np.asarray(input_list)\n",
    "    array_sum = np.sum(input_np_array)\n",
    "    array_has_nan = np.isnan(array_sum)\n",
    "    \n",
    "    return array_has_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "# Extract info into lists\n",
    "text = list(df['text'])\n",
    "target = list(df['target'])\n",
    "location = list(df['location'])\n",
    "\n",
    "# Count of positive vs negative cases\n",
    "pos = 0\n",
    "neg = 0\n",
    "for label in target:\n",
    "    if label == 1:\n",
    "        pos += 1\n",
    "    else: \n",
    "        neg += 1\n",
    "    \n",
    "# Prep to gather tweet data\n",
    "raw_length = 0\n",
    "filtered_length = 0 #i.e. without hashtags\n",
    "ht_count = 0\n",
    "num_tweets_w_hts = 0\n",
    "\n",
    "clean_text_list = []\n",
    "vocab = set()\n",
    "\n",
    "# Initialize lemmatizer, regex_filter, stopwords for preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "regex_filter = re.compile('[^a-zA-Z]')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Gather tweet data and preprocess text\n",
    "for i, sentence in enumerate(text):\n",
    "    \n",
    "    sentence_info_dict = clean_sentence_helper(sentence, lemmatizer, regex_filter, stop_words)\n",
    "    clean_word_list = sentence_info_dict['clean_word_list']\n",
    "    hashtag_count = sentence_info_dict['hashtag_count']\n",
    "    \n",
    "    current_raw_length = len(clean_word_list)\n",
    "    \n",
    "    for word in clean_word_list:\n",
    "        vocab.add(word)\n",
    "    \n",
    "    raw_length += len(sentence_info_dict['raw_word_list'])\n",
    "    filtered_length += len(clean_word_list)\n",
    "    ht_count += hashtag_count\n",
    "    \n",
    "    # Count how many tweets contain hashtags\n",
    "    if hashtag_count > 0:\n",
    "        num_tweets_w_hts += 1\n",
    "    \n",
    "    clean_sentence = \" \".join(clean_word_list)\n",
    "    clean_text_list.append(clean_sentence)\n",
    "\n",
    "num_tweets = len(text)\n",
    "ave_raw_length = raw_length / num_tweets\n",
    "ave_filtered_length = filtered_length / num_tweets\n",
    "ave_ht_count = ht_count / num_tweets  \n",
    "\n",
    "print(\"preprocessing complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output CSV with clean text and lablels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deed reason may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people receive evacuation order california</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>update california hwy closed direction due lak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heavy rain cause flash flooding street manitou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>im top hill see fire wood</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>there emergency evacuation happening building ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>im afraid tornado coming area</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                    deed reason may allah forgive u      1\n",
       "1              forest fire near la ronge sask canada      1\n",
       "2  resident asked shelter place notified officer ...      1\n",
       "3         people receive evacuation order california      1\n",
       "4             got sent photo ruby smoke pours school      1\n",
       "5  update california hwy closed direction due lak...      1\n",
       "6  heavy rain cause flash flooding street manitou...      1\n",
       "7                          im top hill see fire wood      1\n",
       "8  there emergency evacuation happening building ...      1\n",
       "9                      im afraid tornado coming area      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.DataFrame(columns = ['text', 'label']) \n",
    "clean_df['text'] = clean_text_list\n",
    "clean_df['label'] = target\n",
    "\n",
    "clean_df.to_csv(\"output/clean_train_data.csv\")\n",
    " \n",
    "clean_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output CSV for BERT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deed reason may allah forgive u</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people receive evacuation order california</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got sent photo ruby smoke pours school</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>update california hwy closed direction due lak...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heavy rain cause flash flooding street manitou...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>im top hill see fire wood</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>there emergency evacuation happening building ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>im afraid tornado coming area</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0                    deed reason may allah forgive u  positive\n",
       "1              forest fire near la ronge sask canada  positive\n",
       "2  resident asked shelter place notified officer ...  positive\n",
       "3         people receive evacuation order california  positive\n",
       "4             got sent photo ruby smoke pours school  positive\n",
       "5  update california hwy closed direction due lak...  positive\n",
       "6  heavy rain cause flash flooding street manitou...  positive\n",
       "7                          im top hill see fire wood  positive\n",
       "8  there emergency evacuation happening building ...  positive\n",
       "9                      im afraid tornado coming area  positive"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df = pd.DataFrame(columns = ['review', 'sentiment']) \n",
    "bert_df['review'] = clean_text_list\n",
    "bert_df['sentiment'] = target\n",
    "\n",
    "bert_df.replace(to_replace = 1,value = \"positive\", inplace=True)\n",
    "bert_df.replace(to_replace = 0,value = \"negative\", inplace=True)\n",
    "\n",
    "bert_df.to_csv(\"output/bert_train_data.csv\", index=False)\n",
    "\n",
    "bert_df.reset_index(drop=True)\n",
    " \n",
    "bert_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out findings from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_tweets: 7613\n",
      "num_tweets_w_hts: 1761\n",
      "\n",
      "\n",
      "Label distribution ...\n",
      "pos: 3271\n",
      "neg: 4342\n",
      "\n",
      "\n",
      "Fraction of tweets with hashtags: 0.2313148561670826\n",
      "ave_raw_length: 14.903585971364771\n",
      "ave_filtered_length: 8.803625377643504\n",
      "ave_ht_count: 0.4445028241166426\n",
      "\n",
      "\n",
      "Vocabulary size (excluding hashtags): 14404\n"
     ]
    }
   ],
   "source": [
    "print(\"num_tweets: {}\".format(num_tweets))\n",
    "print(\"num_tweets_w_hts: {}\".format(num_tweets_w_hts))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Label distribution ...\")\n",
    "print(\"pos: {}\".format(pos))\n",
    "print(\"neg: {}\".format(neg))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Fraction of tweets with hashtags: {}\".format((num_tweets_w_hts/num_tweets)))\n",
    "print(\"ave_raw_length: {}\".format(ave_raw_length))\n",
    "print(\"ave_filtered_length: {}\".format(ave_filtered_length))\n",
    "print(\"ave_ht_count: {}\".format(ave_ht_count))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Vocabulary size (excluding hashtags): {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate vector representation for each sentence. Split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakshamgandhi/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "tweet_vector_list = []\n",
    "for i, tweet in enumerate(clean_text_list):\n",
    "    sentence_vector = generate_sentence_vector(tweet, w2v_model)\n",
    "    tweet_vector_list.append(sentence_vector)\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_vector_list, target, test_size=0.25, random_state=0)\n",
    "\n",
    "x_train = np.nan_to_num(x_train)\n",
    "x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Baseline Model and Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7773109243697479\n",
      "1904\n",
      "1904\n",
      "sanity check makes sense!\n",
      "accuracy: 0.7773109243697479\n",
      "% predicted pos: 0.3755252100840336\n",
      "precison: 0.7608391608391608\n",
      "recall: 0.6825595984943539\n",
      "f1_score: 0.7195767195767196\n"
     ]
    }
   ],
   "source": [
    "# Define Classifier\n",
    "logistic_regr = LogisticRegression()\n",
    "\n",
    "# Train Classifier and Test\n",
    "logistic_regr.fit(x_train, y_train)\n",
    "predictions = logistic_regr.predict(x_test)\n",
    "score = logistic_regr.score(x_test, y_test)\n",
    "print(score)\n",
    "\n",
    "# Ensure these two numbers match\n",
    "print(len(predictions))\n",
    "print(len(y_test))\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# # clf = GaussianNB()\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "# clf.fit(x_train, y_train)\n",
    "\n",
    "# predictions = clf.predict(x_test)\n",
    "\n",
    "correct = 0\n",
    "predicted_pos = 0\n",
    "true_pos = 0\n",
    "true_neg = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "\n",
    "# Count true pos, true neg, etc\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == y_test[i]:\n",
    "        correct += 1\n",
    "    if predictions[i] == 1:\n",
    "        predicted_pos += 1\n",
    "        if y_test[i] == 1:\n",
    "            true_pos += 1\n",
    "        else:\n",
    "            false_pos += 1\n",
    "    else:\n",
    "        if y_test[i] == 1:\n",
    "            false_neg += 1\n",
    "        else:\n",
    "            true_neg += 1\n",
    "              \n",
    "# Calculate Success Metrics\n",
    "precision = true_pos / (true_pos + false_pos)\n",
    "recall = true_pos / (true_pos + false_neg)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "            \n",
    "# Sanity check\n",
    "if (true_pos + true_neg + false_pos + false_neg) == len(y_test):\n",
    "    print(\"sanity check makes sense!\")\n",
    "        \n",
    "# Print info\n",
    "print(\"accuracy: {}\".format((correct / len(y_test))))\n",
    "print(\"% predicted pos: {}\".format((predicted_pos / len(y_test))))\n",
    "print(\"precison: {}\".format(precision))\n",
    "print(\"recall: {}\".format(recall))\n",
    "print(\"f1_score: {}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    # NOTE: you may have to modify these function headers to include your \n",
    "    # modification, e.g. adding a parameter for embeddings data\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, class_set_size, word_vec_embedding):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, class_set_size):\n",
    "        super(BiLSTMPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # a BiLSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a Linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding()\n",
    "#         self.lstm_layer = nn.LSTM(embedding_dim, \n",
    "#                                   hidden_dim, \n",
    "#                                   num_layers=LSTM_LAYERS,\n",
    "#                                   bidirectional = True,\n",
    "#                                   dropout=DROPOUT\n",
    "#                                   )\n",
    "#         self.embedding_layer = word_vec_embedding\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, \n",
    "                                  hidden_dim, \n",
    "                                  num_layers=LSTM_LAYERS,\n",
    "                                  bidirectional = False,\n",
    "                                  dropout=DROPOUT\n",
    "                                  )\n",
    "        \n",
    "        # Uncomment line below if using biLSTM instead of LSTM\n",
    "#         self.linear_layer = nn.Linear(hidden_dim * 2, class_set_size)\n",
    "        self.linear_layer = nn.Linear(hidden_dim, class_set_size)\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # find the corresponding scores for tags\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "#         print(\"Printing sentence shape for debugging: {}\".format(sentence.shape))\n",
    "        embeddings = self.embedding_layer(sentence)\n",
    "#         print(\"Print embeddings.shape: {}\".format(embeddings.shape))\n",
    "        embeddings = embeddings.view(len(sentence), 1, -1)\n",
    "#         print(\"Printing embeddings shape for debugging: {}\".format(embeddings.shape))\n",
    "        # TODO: Check if we need the other 2 values returned by lstm layer\n",
    "        output, (h, c) = self.lstm_layer(embeddings)\n",
    "        \n",
    "#         print(\"Exploring forward method\")\n",
    "#         print(\"output.shape: {}\".format(output.shape))\n",
    "#         print(\"h.shape: {}\".format(h.shape))\n",
    "#         print(\"c.shape: {}\".format(c.shape))\n",
    "        \n",
    "        # tag_scores = self.linear_layer(tag_scores)\n",
    "        class_scores = self.linear_layer(h.view(1, -1))\n",
    "        \n",
    "#         print(\"Print class_scores: {}\".format(class_scores))\n",
    "\n",
    "        # TODO: Check if this is needed\n",
    "        # tag_scores = F.log_softmax(tag_scores, dim=1)\n",
    "        class_scores = F.softmax(class_scores, dim=1)\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return class_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_function, optimizer, word_vec_model, training_data, val_data):\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, cls in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training loop\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. Find the gradient with respect to the loss and update the\n",
    "        # model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        # Reset every iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Convert to vectors\n",
    "        word_vector_tensor = prepare_sequence(sentence, word_vec_model)\n",
    "        \n",
    "        # If word vector tensor is empty, skip this iteration. Move on to next sentence\n",
    "        if word_vector_tensor == None:\n",
    "            continue\n",
    "#         true_tags_tensor = prepare_sequence(tags, word_vec_model)\n",
    "        true_class_tensor = torch.tensor([cls], dtype=torch.int64)\n",
    "\n",
    "        # Forward prop\n",
    "        cls_prediction_tensor = model(word_vector_tensor)\n",
    "#         predicted_cls_tensor = torch.argmax(cls_prediction_tensor, dim=1)\n",
    "\n",
    "#         print(\"Printing cls_prediction_tensor: {}\".format(cls_prediction_tensor))\n",
    "#         print(\"Printing true_class_tensor: {}\".format(true_class_tensor))\n",
    "\n",
    "        # Backward Prop\n",
    "#         print(\"cls: {}\".format(cls))\n",
    "#         print(\"cls_prediction_tensor.shape: {}\".format(cls_prediction_tensor.shape))\n",
    "#         print(\"true_class_tensor: {}\".format(true_class_tensor))\n",
    "#         print(\"cls_prediction_tensor: {}\".format(cls_prediction_tensor))\n",
    "#         print(\"predicted_cls_tensor: {}\".format(predicted_cls_tensor))\n",
    "#         print(\"predicted_cls_tensor.dtype: {}\".format(predicted_cls_tensor.dtype))\n",
    "        loss = loss_function(cls_prediction_tensor, true_class_tensor)\n",
    "      \n",
    "        # Update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss\n",
    "#         train_examples += len(tags)\n",
    "        train_examples += 1\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    avg_train_loss = None\n",
    "    if train_examples > 0:\n",
    "        avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate(\n",
    "                                    model, \n",
    "                                    loss_function, \n",
    "                                    optimizer,\n",
    "                                    word_vec_model,\n",
    "                                    val_data\n",
    "                                )\n",
    "    \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate(model, loss_function, optimizer, word_vec_model, val_data):\n",
    "  # returns:: avg_val_loss (float)\n",
    "  # returns:: val_accuracy (float)\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, cls in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate loop\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            \n",
    "            # Convert to vectors\n",
    "            word_vector_tensor = prepare_sequence(sentence, word_vec_model)\n",
    "#             true_tags_tensor = prepare_sequence(tags, tag_to_idx)\n",
    "            true_class_tensor = torch.tensor([cls], dtype=torch.int64)\n",
    "\n",
    "            # Forward prop\n",
    "            cls_prediction_tensor = model(word_vector_tensor)\n",
    "            predicted_tags_tensor = torch.argmax(cls_prediction_tensor, dim=1)\n",
    "\n",
    "            # Analysis\n",
    "            loss = loss_function(cls_prediction_tensor, true_class_tensor)\n",
    "            comparison_tensor = predicted_tags_tensor == true_class_tensor\n",
    "            \n",
    "            print(\"predicted_tags_tensor: {}\".format(predicted_tags_tensor))\n",
    "            print(\"true_class_tensor: {}\".format(true_class_tensor))\n",
    "            import time\n",
    "            time.sleep(10000)\n",
    "\n",
    "            val_loss += loss\n",
    "#             val_examples += len(tags)\n",
    "            val_examples += 1\n",
    "            correct += torch.sum(comparison_tensor)\n",
    "\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_tags_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-92c7bcca5160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predicted_tags_tensor: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_tags_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"true_class_tensor: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_class_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_tags_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"predicted_tags_tensor: {}\".format(predicted_tags_tensor))\n",
    "print(\"true_class_tensor: {}\".format(true_class_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for BiLSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_list = clean_df['text'].tolist()\n",
    "target = clean_df['label'].tolist()\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(clean_text_list, target, test_size=0.25, random_state=0)\n",
    "\n",
    "# Return lists, not iterables. Otherwise need to run this cell over and over again\n",
    "training_data = []\n",
    "for x, y in zip(x_train, y_train):\n",
    "    training_data.append((x, y))\n",
    "    \n",
    "val_data = []\n",
    "for x, y in zip(x_test, y_test):\n",
    "    val_data.append((x, y))\n",
    "\n",
    "# x_train = np.nan_to_num(x_train)\n",
    "# x_test = np.nan_to_num(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters for BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = w2v_embedding.embedding_dim\n",
    "HIDDEN_DIM = 15\n",
    "# LEARNING_RATE = 0.005\n",
    "LEARNING_RATE = 0.00005\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 30\n",
    "CLASS_SET_SIZE = 2 # Only 2 classes of possible tweets\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize NN, optimizer and loss function object. Start training and evluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'num_embeddings' and 'embedding_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-b26a9c697a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiLSTMPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLASS_SET_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# model = BiLSTMPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, CLASS_SET_SIZE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-206-3f347ea3817d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding_dim, hidden_dim, class_set_size, word_vec_embedding)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#         self.lstm_layer = nn.LSTM(embedding_dim,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#                                   hidden_dim,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'num_embeddings' and 'embedding_dim'"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "\n",
    "model = BiLSTMPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, CLASS_SET_SIZE, w2v_embedding)\n",
    "# model = BiLSTMPOSTagger(EMBEDDING_DIM, HIDDEN_DIM, CLASS_SET_SIZE)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "# TODO: Check which loss function to use, check if softmax required in forward()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = F.cross_entropy\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train(epoch, model, loss_function, optimizer, w2v_model, training_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cells from here onwards are for random testing. Treat as a Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "tensor([[ 0.1309,  0.0084,  0.0334, -0.0588,  0.0400, -0.1426,  0.0493, -0.1689,\n",
      "          0.2090,  0.1196,  0.1807, -0.2500, -0.1040, -0.1074, -0.0188,  0.0520,\n",
      "         -0.0022,  0.0645,  0.1445, -0.0454,  0.1611, -0.0161, -0.0309,  0.0845,\n",
      "          0.1621,  0.0447, -0.1553,  0.2539,  0.3398,  0.0076, -0.2559, -0.0173,\n",
      "         -0.0330,  0.1631, -0.1260, -0.0991,  0.1650,  0.0688, -0.1895,  0.0283,\n",
      "         -0.0535, -0.0306,  0.1108,  0.2412, -0.2344,  0.1235, -0.0029,  0.1484,\n",
      "          0.3320,  0.0525, -0.2002,  0.3770,  0.1226,  0.1143, -0.1768,  0.1001,\n",
      "          0.0030,  0.2676,  0.2012,  0.0371,  0.1108, -0.0981, -0.3125,  0.0352,\n",
      "          0.0283,  0.2617, -0.0864, -0.0226, -0.0583, -0.0079,  0.1177, -0.0430,\n",
      "         -0.1729,  0.0439, -0.2305,  0.1641, -0.1147, -0.0603,  0.0120, -0.2471,\n",
      "          0.3262, -0.0449, -0.1143,  0.2285, -0.0165, -0.1504, -0.1318,  0.1260,\n",
      "         -0.1748,  0.0221, -0.1016,  0.0082,  0.1079, -0.2461, -0.1094, -0.0938,\n",
      "         -0.0162, -0.2021,  0.2314, -0.0544, -0.0554, -0.2090,  0.2676,  0.2793,\n",
      "          0.1709, -0.1758, -0.0277, -0.2041,  0.0239,  0.0312, -0.2539, -0.1250,\n",
      "         -0.0549, -0.1738,  0.2852, -0.2324,  0.0234, -0.2012, -0.1348,  0.2637,\n",
      "          0.0077,  0.2051, -0.0171, -0.1299,  0.0471,  0.2207,  0.0210, -0.2910,\n",
      "         -0.0289,  0.1729,  0.0427, -0.1982, -0.0400, -0.1699,  0.1006, -0.0933,\n",
      "          0.1582, -0.1650, -0.0605,  0.1943, -0.0708, -0.0688, -0.0962, -0.0723,\n",
      "          0.0488,  0.0732,  0.1104,  0.0486, -0.1768, -0.3379,  0.2256,  0.1631,\n",
      "          0.0510, -0.0825,  0.0796,  0.0874, -0.1689, -0.0216, -0.1924,  0.0386,\n",
      "         -0.0510,  0.2197,  0.0801, -0.2119, -0.0752, -0.1504,  0.3047, -0.1709,\n",
      "          0.1235, -0.2344, -0.1074, -0.0679,  0.0190, -0.1416, -0.2275, -0.1631,\n",
      "          0.1445, -0.1514, -0.2969,  0.2236, -0.1021, -0.0457, -0.2168, -0.0903,\n",
      "          0.0938, -0.1533, -0.0155,  0.3047, -0.2373,  0.0894,  0.0371,  0.0294,\n",
      "         -0.2852,  0.1582, -0.0031,  0.0605,  0.0050, -0.1523, -0.0084,  0.0220,\n",
      "         -0.1211, -0.1387, -0.2734, -0.0684,  0.0825, -0.2637, -0.1699,  0.1475,\n",
      "          0.0850,  0.0208,  0.1367, -0.0493, -0.0101, -0.0037, -0.1084,  0.1475,\n",
      "         -0.1553,  0.1611,  0.0562, -0.0500, -0.1641, -0.2695,  0.4141,  0.0608,\n",
      "         -0.0469, -0.0251,  0.1060,  0.1328, -0.1670, -0.0491,  0.0466,  0.0515,\n",
      "         -0.0796, -0.1650, -0.2988,  0.0605, -0.1533, -0.0060,  0.0664, -0.0452,\n",
      "          0.2432, -0.0708, -0.3691, -0.2314, -0.1191, -0.0830,  0.1475, -0.0576,\n",
      "          0.2354, -0.1230,  0.1465,  0.1367,  0.1543,  0.0211, -0.0957,  0.0586,\n",
      "          0.0398, -0.0811,  0.0559, -0.1660,  0.2715, -0.2012, -0.0092,  0.0732,\n",
      "          0.1045,  0.3457, -0.2637,  0.0210, -0.4004, -0.0342, -0.1592, -0.0879,\n",
      "          0.0820,  0.2334,  0.0214, -0.1133,  0.0525, -0.1045, -0.0238, -0.0835,\n",
      "         -0.0400,  0.0192, -0.0123, -0.1826, -0.0679, -0.0850, -0.0304, -0.0540,\n",
      "          0.0425,  0.1279, -0.2754,  0.2852, -0.0474,  0.0649, -0.1123, -0.0258,\n",
      "         -0.0413,  0.2285, -0.1494, -0.1504]])\n"
     ]
    }
   ],
   "source": [
    "car_index = w2v_model.vocab['car'].index\n",
    "print(car_index)\n",
    "\n",
    "input = torch.LongTensor([car_index])\n",
    "print(embedding(input))\n",
    "\n",
    "\n",
    "# lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "# hello_embed = embeds(lookup_tensor)\n",
    "# print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13085938  0.00842285  0.03344727 -0.05883789  0.04003906 -0.14257812\n",
      "  0.04931641 -0.16894531  0.20898438  0.11962891  0.18066406 -0.25\n",
      " -0.10400391 -0.10742188 -0.01879883  0.05200195 -0.00216675  0.06445312\n",
      "  0.14453125 -0.04541016  0.16113281 -0.01611328 -0.03088379  0.08447266\n",
      "  0.16210938  0.04467773 -0.15527344  0.25390625  0.33984375  0.00756836\n",
      " -0.25585938 -0.01733398 -0.03295898  0.16308594 -0.12597656 -0.09912109\n",
      "  0.16503906  0.06884766 -0.18945312  0.02832031 -0.0534668  -0.03063965\n",
      "  0.11083984  0.24121094 -0.234375    0.12353516 -0.00294495  0.1484375\n",
      "  0.33203125  0.05249023 -0.20019531  0.37695312  0.12255859  0.11425781\n",
      " -0.17675781  0.10009766  0.0030365   0.26757812  0.20117188  0.03710938\n",
      "  0.11083984 -0.09814453 -0.3125      0.03515625  0.02832031  0.26171875\n",
      " -0.08642578 -0.02258301 -0.05834961 -0.00787354  0.11767578 -0.04296875\n",
      " -0.17285156  0.04394531 -0.23046875  0.1640625  -0.11474609 -0.06030273\n",
      "  0.01196289 -0.24707031  0.32617188 -0.04492188 -0.11425781  0.22851562\n",
      " -0.01647949 -0.15039062 -0.13183594  0.12597656 -0.17480469  0.02209473\n",
      " -0.1015625   0.00817871  0.10791016 -0.24609375 -0.109375   -0.09375\n",
      " -0.01623535 -0.20214844  0.23144531 -0.05444336 -0.05541992 -0.20898438\n",
      "  0.26757812  0.27929688  0.17089844 -0.17578125 -0.02770996 -0.20410156\n",
      "  0.02392578  0.03125    -0.25390625 -0.125      -0.05493164 -0.17382812\n",
      "  0.28515625 -0.23242188  0.0234375  -0.20117188 -0.13476562  0.26367188\n",
      "  0.00769043  0.20507812 -0.01708984 -0.12988281  0.04711914  0.22070312\n",
      "  0.02099609 -0.29101562 -0.02893066  0.17285156  0.04272461 -0.19824219\n",
      " -0.04003906 -0.16992188  0.10058594 -0.09326172  0.15820312 -0.16503906\n",
      " -0.06054688  0.19433594 -0.07080078 -0.06884766 -0.09619141 -0.07226562\n",
      "  0.04882812  0.07324219  0.11035156  0.04858398 -0.17675781 -0.33789062\n",
      "  0.22558594  0.16308594  0.05102539 -0.08251953  0.07958984  0.08740234\n",
      " -0.16894531 -0.02160645 -0.19238281  0.03857422 -0.05102539  0.21972656\n",
      "  0.08007812 -0.21191406 -0.07519531 -0.15039062  0.3046875  -0.17089844\n",
      "  0.12353516 -0.234375   -0.10742188 -0.06787109  0.01904297 -0.14160156\n",
      " -0.22753906 -0.16308594  0.14453125 -0.15136719 -0.296875    0.22363281\n",
      " -0.10205078 -0.0456543  -0.21679688 -0.09033203  0.09375    -0.15332031\n",
      " -0.01550293  0.3046875  -0.23730469  0.08935547  0.03710938  0.02941895\n",
      " -0.28515625  0.15820312 -0.00306702  0.06054688  0.00497437 -0.15234375\n",
      " -0.00836182  0.02197266 -0.12109375 -0.13867188 -0.2734375  -0.06835938\n",
      "  0.08251953 -0.26367188 -0.16992188  0.14746094  0.08496094  0.02075195\n",
      "  0.13671875 -0.04931641 -0.0100708  -0.00369263 -0.10839844  0.14746094\n",
      " -0.15527344  0.16113281  0.05615234 -0.05004883 -0.1640625  -0.26953125\n",
      "  0.4140625   0.06079102 -0.046875   -0.02514648  0.10595703  0.1328125\n",
      " -0.16699219 -0.04907227  0.04663086  0.05151367 -0.07958984 -0.16503906\n",
      " -0.29882812  0.06054688 -0.15332031 -0.00598145  0.06640625 -0.04516602\n",
      "  0.24316406 -0.07080078 -0.36914062 -0.23144531 -0.11914062 -0.08300781\n",
      "  0.14746094 -0.05761719  0.23535156 -0.12304688  0.14648438  0.13671875\n",
      "  0.15429688  0.02111816 -0.09570312  0.05859375  0.03979492 -0.08105469\n",
      "  0.0559082  -0.16601562  0.27148438 -0.20117188 -0.00915527  0.07324219\n",
      "  0.10449219  0.34570312 -0.26367188  0.02099609 -0.40039062 -0.03417969\n",
      " -0.15917969 -0.08789062  0.08203125  0.23339844  0.0213623  -0.11328125\n",
      "  0.05249023 -0.10449219 -0.02380371 -0.08349609 -0.04003906  0.01916504\n",
      " -0.01226807 -0.18261719 -0.06787109 -0.08496094 -0.03039551 -0.05395508\n",
      "  0.04248047  0.12792969 -0.27539062  0.28515625 -0.04736328  0.06494141\n",
      " -0.11230469 -0.02575684 -0.04125977  0.22851562 -0.14941406 -0.15039062]\n"
     ]
    }
   ],
   "source": [
    "print(model['car'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
